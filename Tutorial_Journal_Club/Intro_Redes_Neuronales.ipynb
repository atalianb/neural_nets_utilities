{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a las redes neuronales con ejemplos numéricos\n",
    "\n",
    "- Diagrama general del aprendizajo supervisado en ML:\n",
    "\n",
    "![title](https://volcanohong.github.io/content/images/2016/ml_process.png)\n",
    "\n",
    "- ¿Qué es una red neuronal?\n",
    "\n",
    "Según Wiki: es un modelo computacional vagamente inspirado en el comportamiento observado en su homólogo biológico.\n",
    "\n",
    "![title](data:image/png;base64,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)\n",
    "\n",
    "- ¿De qué sirve una sola neurona?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import tensorflow.keras as K\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para explicar cada parte, usaremos la implementación de Michael Nielsen en su libro Neural Networks and Deep Learning:\n",
    "\n",
    "- Libro de acceso abierto: http://neuralnetworksanddeeplearning.com\n",
    "- La presente versión ha sido modificada para fines didácticos:\n",
    "    - Traducida al español.\n",
    "    - Adaptada para regresión.\n",
    "    - Adaptada a Python 3.\n",
    "\n",
    "Consíderese lo siguiente:\n",
    "\n",
    "- Cada conexión tiene un peso.\n",
    "- Cada neurona tiene un bias. \n",
    "- Función de costo: \n",
    "$C(w, b) ≡ \\frac{1}{2} \\Sigma_x || y(x) − a(x,w,b)||^2$, donde a es la predicción de la RNA.\n",
    "- Métodos analíticos de minimización no son útiles para muchas variables.\n",
    "- Se requiere un algoritmo para minimizar la función de costo: descenso del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "            size es una lista que \n",
    "            debe contener el número de neuronas para cada una de las tres\n",
    "            paredes de la red neuronal\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        print(\"Forma del np.array con bias (uno por cada conexión): {}\".format(np.shape(self.biases)))\n",
    "        print(self.biases)\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        print(\"Forma del np.array de pesos: {}\".format(np.shape(self.weights)))\n",
    "        print(self.weights)\n",
    "\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "           Propagación hacia adelante: realiza la suma ponderada a través de las capas.\n",
    "           Regresa la salida de la red neuronal.\n",
    "           a es la entrada de la red neuronal.\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"\n",
    "        Stochastic Gradient Descent: Desenso del gradiente estocástico.\n",
    "        \n",
    "        - trainig_data: conjunto de entrenamiento, x var independientes, y salida esperada\n",
    "        (var dependiente).\n",
    "        - epochs: número de épocas.\n",
    "        - mini_batch_size: \n",
    "        - eta: learning rate\n",
    "        \"\"\"\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\").format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print (\"Epoch {0} complete\").format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evalúa rendimiento de la red neuronal\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Derivadas parciales\n",
    "        \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del np.array con bias (uno por cada conexión): (2,)\n",
      "[array([[ 1.3893499 ],\n",
      "       [ 0.03773412],\n",
      "       [ 0.87777073],\n",
      "       [-1.29035399],\n",
      "       [ 0.62150626],\n",
      "       [ 0.45876255],\n",
      "       [ 0.24622552],\n",
      "       [ 0.31529999],\n",
      "       [-1.4907041 ],\n",
      "       [ 0.4296312 ]]), array([[0.44688886]])]\n",
      "Forma del np.array de pesos: (2,)\n",
      "[array([[ 0.15211633,  0.92446259],\n",
      "       [-0.54863251,  0.40768956],\n",
      "       [ 0.1186077 , -0.78834051],\n",
      "       [-0.23009001, -0.34078913],\n",
      "       [ 0.264081  , -1.21091026],\n",
      "       [ 0.31580243,  0.4014537 ],\n",
      "       [ 0.02581296, -1.52054939],\n",
      "       [ 1.98471474,  2.35033424],\n",
      "       [ 0.70012123, -1.25257547],\n",
      "       [-1.44687261,  0.27042355]]), array([[-0.48108431,  1.877022  ,  0.53341453, -0.89379251, -2.30113523,\n",
      "        -0.85283299, -0.24365736, -0.51302387,  0.5164416 , -0.50213275]])]\n"
     ]
    }
   ],
   "source": [
    "net = Network([2,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2,3,4,5,6],[2,4,6,8,10,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-dcef893da65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-d7a86ec9f292>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 for k in range(0, n, mini_batch_size)]\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 print(\"Epoch {0}: {1} / {2}\").format(\n",
      "\u001b[0;32m<ipython-input-63-d7a86ec9f292>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, eta)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data=data, epochs=10, eta=0.5, mini_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
