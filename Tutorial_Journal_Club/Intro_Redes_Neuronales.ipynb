{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a las redes neuronales con ejemplos numéricos\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "#### Diagrama general del aprendizajo supervisado en ML\n",
    "\n",
    "![title](https://raw.githubusercontent.com/igomezv/neural_nets_utilities/master/Tutorial_Journal_Club/figures/aprendizajesupervizado.png)\n",
    "\n",
    "#### ¿Qué es una red neuronal?\n",
    "\n",
    "Según Wiki: es un modelo computacional vagamente inspirado en el comportamiento observado en su homólogo biológico.\n",
    "\n",
    " -Numeros inicialmente aleatorios $\\in (0,1)$ asociados a cada conexión (llamados pesos) hacen alusión a la sinapsis.\n",
    " \n",
    " -Cada nodo de la RNA también es llamada neurona.\n",
    " \n",
    " -Hay una función de activación que determina si las señales activan o no cierta respuesta.\n",
    " \n",
    " -Las RNA procesan señales en paralelo.\n",
    "\n",
    "#### Propagación hacia adelante:\n",
    "\n",
    "- Genera pesos y bias aleatorios.\n",
    "- Las entradas de la capa anterior se multiplican por los pesos por medio de una suma ponderada.\n",
    "- Al resultado de la suma ponderada se le aplica una función de activación.\n",
    "- La red neuronal arroja una predicción.\n",
    "\n",
    "![title](https://raw.githubusercontent.com/igomezv/neural_nets_utilities/master/Tutorial_Journal_Club/figures/ANN_.png)\n",
    "\n",
    "#### Tipos de funciones de activación:\n",
    "Fuente: https://mlfromscratch.com/activation-functions-explained/#/\n",
    "![title](https://mlfromscratch.com/content/images/2019/12/activation-functions.gif)\n",
    "\n",
    "Tómese en cuenta lo siguiente.\n",
    "\n",
    "- Los bias y pesos son inicialmente aleatorios, pero de alguna manera se deben ajustar mediante iteraciones.\n",
    "- Cada conexión tiene un peso.\n",
    "- Cada neurona tiene un bias. \n",
    "\n",
    "#### ¿Qué hace falta?\n",
    "- Se requiere una métrica que mida qué tan cerca o lejos del valor esperado son las predicciones de la red (función de costo o de pérdida).\n",
    "- Es necesario un algoritmo que minimize la función anterior.\n",
    "- Si no se emplea una función de activación no lineal, la suma ponderada y toda la estructura de la red solamente proveerán transformaciones lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para explicar cada parte, usaremos la implementación de Michael Nielsen en su libro Neural Networks and Deep Learning:\n",
    "\n",
    "- Libro de acceso abierto: http://neuralnetworksanddeeplearning.com\n",
    "- La presente versión ha sido modificada para fines didácticos:\n",
    "    - Traducida al español.\n",
    "    - Adaptada para regresión.\n",
    "    - Adaptada a Python 3.\n",
    "\n",
    "Consíderese lo siguiente:\n",
    "\n",
    "\n",
    "- Función de costo: \n",
    "$C(w, b) ≡ \\frac{1}{2} \\Sigma_x || y(x) − a(x,w,b)||^2$, donde a es la predicción de la RNA.\n",
    "- Métodos analíticos de minimización no son útiles para muchas variables.\n",
    "- Se requiere un algoritmo para minimizar la función de costo: descenso del gradiente.\n",
    "- El descenso del gradiente únicamente calcula gradientes, es el más simple de toda una familia de algoritmos de minimización.\n",
    "\n",
    "#### Descenso del gradiente (intuición)\n",
    "\n",
    "- La deducción no es propósito de esta notebook, pero cada nuevo paso en busca de la minimización de la función de costo, obedece la siguiente regla:\n",
    "\n",
    "$ v -> v' = v - \\eta \\nabla C$, donde $v$ es $f(w, b)$\n",
    "\n",
    "- Se da un paso en dirección opuesta al gradiente hasta minimizar la función de costo.\n",
    "\n",
    "Fuente: https://kevinbinz.com/2019/05/26/intro-gradient-descent/\n",
    "\n",
    "![title](https://kevinbinz.files.wordpress.com/2019/05/dualspace_explore.gif)\n",
    "\n",
    "- Se quiere encontrar (x,y) tal que sea un mínimo en la superficie (función de costo).\n",
    "\n",
    "Fuente: https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\n",
    "\n",
    "![title](https://miro.medium.com/max/819/1*hUd744hDEEGx0-ypWGhrkw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/isidro/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    def __init__(self, X, Y, topology, epochs=50, lr=0.0001, bs=4, early_tol=100, scale=False):\n",
    "        self.topology = topology\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.early_tol = early_tol\n",
    "        self.scale = scale\n",
    "    \n",
    "        ntrain = int(0.8* len(X))\n",
    "        indx = [ntrain]\n",
    "        self.X_train, self.X_test = np.split(X, indx)\n",
    "        self.Y_train, self.Y_test = np.split(Y, indx)\n",
    "        \n",
    "        if self.scale:\n",
    "            self.scaler = StandardScaler()\n",
    "            try:\n",
    "                self.scaler.fit(X)\n",
    "                self.X_train = self.scaler.transform(self.X_train)\n",
    "                self.X_test = self.scaler.transform(self.X_test)\n",
    "            except:\n",
    "                self.scaler.fit(X.reshape(-1,1))\n",
    "                self.X_train = self.scaler.transform(self.X_train.reshape(-1,1))\n",
    "                self.X_test = self.scaler.transform(self.X_test.reshape(-1,1))\n",
    "\n",
    "            self.X_train = self.scaler.transform(self.X_train)\n",
    "            self.X_test = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        self.model = self.model()\n",
    "        self.model.summary()\n",
    "        \n",
    "    def model(self):\n",
    "        # Red neuronal\n",
    "        model = K.models.Sequential()\n",
    "        # Hidden layers\n",
    "\n",
    "        for i, nodes in enumerate(self.topology):\n",
    "            if i == 0:\n",
    "                model.add(K.layers.Dense(self.topology[1], input_dim=self.topology[0], activation='relu'))\n",
    "            elif i+1 < len(self.topology):\n",
    "                model.add(K.layers.Dense(self.topology[i+1], activation='relu'))\n",
    "        optimizer = K.optimizers.Adam(learning_rate=0.0001)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        \n",
    "        return model                    \n",
    "    \n",
    "    def train(self):\n",
    "        callbacks = [K.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                               min_delta=0.0,\n",
    "                                               patience=self.early_tol,\n",
    "                                               restore_best_weights=True)]\n",
    "        print(\"Entrenando, por favor, espera...\")\n",
    "        self.history = self.model.fit(self.X_train,\n",
    "                                      self.Y_train,\n",
    "                                      validation_data=(self.X_test,\n",
    "                                                       self.Y_test),\n",
    "                                      epochs=self.epochs, batch_size=self.bs,\n",
    "                                      callbacks=callbacks, verbose=0)\n",
    "        print(\"¡Entrenamiento terminado!\")\n",
    "        return self.history\n",
    "    \n",
    "    def get_w_and_b(self, nlayer):\n",
    "        weights, biases = self.model.layers[nlayer].get_weights()\n",
    "        return weights, biases\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if type(x) == type([1]):\n",
    "            x = np.array(x)\n",
    "        if type(x) == type(1):\n",
    "            x = np.array([x])\n",
    "        if self.scale:\n",
    "            try:\n",
    "                x = self.scaler.transform(x)\n",
    "            except:\n",
    "                x = self.scaler.transform(x.reshape(-1,1))\n",
    "        return self.model.predict(x)\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot(self.history.history['loss'], label='training set')\n",
    "        plt.plot(self.history.history['val_loss'], label='validation set')\n",
    "        plt.ylabel('loss function')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(x):\n",
    "    return 2*x + 3 + 0.5 * np.random.rand()\n",
    "\n",
    "def quadratic(x):\n",
    "    return x**2 + 0.5 * np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.347842</td>\n",
       "      <td>2.026589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.279041</td>\n",
       "      <td>53.194355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.388418</td>\n",
       "      <td>5.914453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.901272</td>\n",
       "      <td>79.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.154524</td>\n",
       "      <td>1.542836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.938065</td>\n",
       "      <td>48.346658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.353514</td>\n",
       "      <td>5.748940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.748393</td>\n",
       "      <td>7.763577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.596710</td>\n",
       "      <td>57.919907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.960021</td>\n",
       "      <td>63.571852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x          y\n",
       "0  1.347842   2.026589\n",
       "1  7.279041  53.194355\n",
       "2  2.388418   5.914453\n",
       "3  8.901272  79.442545\n",
       "4  1.154524   1.542836\n",
       "5  6.938065  48.346658\n",
       "6  2.353514   5.748940\n",
       "7  2.748393   7.763577\n",
       "8  7.596710  57.919907\n",
       "9  7.960021  63.571852"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npoints = 10000\n",
    "X = np.random.rand(npoints)*10\n",
    "Y = quadratic(X)\n",
    "data =pd.DataFrame(zip(X,Y), columns=['x', 'y'])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 1000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = neural_net(X, Y, [1,1000,1], epochs=50, bs=16, lr=0.1, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando, por favor, espera...\n"
     ]
    }
   ],
   "source": [
    "network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = network.get_w_and_b(nlayer=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = network.predict([0.5, 5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
