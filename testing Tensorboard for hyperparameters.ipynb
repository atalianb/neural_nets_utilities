{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/isidro/Desktop/chains_server/chains_1000/chains/owaCDM_phy_HD+SN+BBAO+Planck_nested_dynesty_multi_1.txt\"\n",
    "\n",
    "split = 0.8\n",
    "numNeurons = 500\n",
    "epochs = 300\n",
    "nlayers = 2\n",
    "batch_size = 0\n",
    "\n",
    "params = np.loadtxt(file, usecols=(2,3,4,5,6))\n",
    "logL = np.loadtxt(file, usecols=(1))\n",
    "\n",
    "nparams = len(params)\n",
    "randomize = np.random.permutation(nparams)\n",
    "params = params[randomize]\n",
    "logL = logL[randomize]\n",
    "\n",
    "maxLogL = np.max(logL)\n",
    "minLogL = np.min(logL)\n",
    "ntrain = int(split * nparams)\n",
    "indx = [ntrain]\n",
    "# xtrain <- params_training ytrain<-logLtraining\n",
    "params_training, params_testing = np.split(params, indx)\n",
    "logL_training, logL_testing = np.split(logL, indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=1e-3,\n",
    "                                   patience=5,\n",
    "                                   restore_best_weights=True)]\n",
    "\n",
    "n_cols = params_training.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric('loss', display_name=\"Loss\")])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu, input_shape=(n_cols,)),\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "                              \n",
    "    model.compile(\n",
    "        optimizer=hparams[HP_OPTIMIZER],\n",
    "        loss='mean_squared_error', metrics=['mean_squared_error']\n",
    "      )\n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "    # model.fit(x_train, y_train, epochs=1, callbacks=callbacks) \n",
    "    # _, loss = model.evaluate(x_test, y_test)\n",
    "    model.fit(params_training, logL_training, epochs=100, callbacks=callbacks) \n",
    "    _, loss = model.evaluate(params_testing, logL_testing)\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    loss = train_test_model(hparams)\n",
    "    tf.summary.scalar(\"loss\", loss, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "Epoch 1/100\n",
      "522/557 [===========================>..] - ETA: 0s - loss: 6768122368.0000 - mean_squared_error: 6768122368.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 679us/step - loss: 6690221056.0000 - mean_squared_error: 6690221056.0000\n",
      "Epoch 2/100\n",
      "545/557 [============================>.] - ETA: 0s - loss: 6690575872.0000 - mean_squared_error: 6690575872.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 746us/step - loss: 6624987648.0000 - mean_squared_error: 6624987648.0000\n",
      "Epoch 3/100\n",
      "539/557 [============================>.] - ETA: 0s - loss: 6525331456.0000 - mean_squared_error: 6525331456.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 841us/step - loss: 6561214464.0000 - mean_squared_error: 6561214464.0000\n",
      "Epoch 4/100\n",
      "522/557 [===========================>..] - ETA: 0s - loss: 6310971392.0000 - mean_squared_error: 6310971392.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 776us/step - loss: 6539017216.0000 - mean_squared_error: 6539017216.0000\n",
      "Epoch 5/100\n",
      "545/557 [============================>.] - ETA: 0s - loss: 6550952448.0000 - mean_squared_error: 6550952448.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 838us/step - loss: 6522632192.0000 - mean_squared_error: 6522632192.0000\n",
      "Epoch 6/100\n",
      "513/557 [==========================>...] - ETA: 0s - loss: 6633105408.0000 - mean_squared_error: 6633105408.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 794us/step - loss: 6505781248.0000 - mean_squared_error: 6505781248.0000\n",
      "Epoch 7/100\n",
      "542/557 [============================>.] - ETA: 0s - loss: 6557876224.0000 - mean_squared_error: 6557876224.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 1s 932us/step - loss: 6489205248.0000 - mean_squared_error: 6489205248.0000\n",
      "Epoch 8/100\n",
      "522/557 [===========================>..] - ETA: 0s - loss: 6476802048.0000 - mean_squared_error: 6476802048.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_squared_error\n",
      "557/557 [==============================] - 0s 771us/step - loss: 6471800832.0000 - mean_squared_error: 6471800832.0000\n",
      "Epoch 9/100\n",
      "414/557 [=====================>........] - ETA: 0s - loss: 6550414336.0000 - mean_squared_error: 6550414336.0000"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_DROPOUT: dropout_rate,\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "      }\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      run('logs/hparam_tuning/' + run_name, hparams)\n",
    "      session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
